syntax = "proto2";

package cnn;

message ArrayProto
{
    optional int32 n = 1;       // number of batches
    optional int32 c = 2;       // number of channels
    optional int32 h = 3;       // number of rows, i.e., height
    optional int32 w = 4;       // number of columns, i.e., width
    repeated double d = 5;      // data
}

message OptimizerProto
{
    optional string model_filename = 1;
    optional double learning_rate = 2;
    optional int32 max_iteration_num = 3;   // maximum number of iterations
    optional int32 print_interval    = 4 [default = 1000];   // print after every multiple of this number of iterations
    optional string trained_filename  = 5;  // a binary file; if present, copy weights from it
    optional int32 snapshot_interval  = 6;  // save trained weights after every multiple of this number of iterations
    optional string snapshot_prefix   = 7;  // prefix of the snapshot
}

message NetworkProto
{
    repeated LayerProto layer_proto = 1;
}

enum LayerType
{
    INPUT           = 0;        // input layer
    FULL_CONNECTED  = 1;        // full connected layer
    L2_LOSS         = 2;        // l2 loss layer, for regression only
    SOFTMAX         = 3;
    LOG_LOSS        = 4;
    SOFTMAX_WITH_LOG_LOSS = 5;
    CONVOLUTION     = 6;
    RELU            = 7;
    MAX_POOLING     = 8;
    DROP_OUT        = 9;
    BATCH_NORMALIZATION = 10;
}

// same as caffe
enum Phase
{
    TRAIN = 0;  // during the train phase, we have to allocate gradient
    TEST  = 1;  // during the test case, no gradient is needed
}

message LayerProto
{
    optional string name        = 1;    // can be arbitrary and may be not unique, for debug only
    optional LayerType type     = 2;
    repeated string bottom      = 3;    // bottom/top name are globally unique
    repeated string top         = 4;    // bottom/top name are globally unique
    optional Phase phase        = 5;    // train or test, used from gradient space allocation
    repeated ArrayProto param   = 6;    // parameters for this layer

    optional InputLayerProto input_proto        = 7;
    optional FullConnectedLayerProto fc_proto   = 8;
    optional ConvolutionLayerProto conv_proto   = 9;
    optional MaxPoolingLayerProto max_pooling_proto = 10;
    optional DropoutLayerProto dropout_proto    = 11;
}

message InputLayerProto
{
    optional int32 n = 1;
    optional int32 c = 2;
    optional int32 h = 3;
    optional int32 w = 4;
}

message FullConnectedLayerProto
{
    optional int32 num_output = 1;  // number of outputs
}

message ConvolutionLayerProto
{
    optional int32 num_output = 1;
    optional int32 kernel_size = 2;   // size of the square kernel
    // currently we assume implicit padding with stride 1 so
    // that the output size equals to the input size
}

message MaxPoolingLayerProto
{
    optional int32 win_size  = 1;  // size of a square window
    optional int32 stride = 2;  // stride of the window
}

message DropoutLayerProto
{
    optional double keep_prob = 1;  // the probability to retain the output
}

