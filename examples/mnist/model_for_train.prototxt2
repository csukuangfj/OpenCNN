
layer_proto {
    name: "input"
    type: INPUT
    top: "data"
    top: "label"
    input_proto {
        n: 10
        c: 1
        h: 28
        w: 28
    }
}

layer_proto {
    name: "conv1"
    type: CONVOLUTION
    bottom: "data"
    top: "conv1"
    conv_proto {
        num_output: 32
        kernel_size: 3
    }
}

layer_proto {
    name: "bn1"
    type: BATCH_NORMALIZATION
    bottom: "conv1"
    top: "bn1"
}

layer_proto {
    name: "relu1"
    type: RELU
    bottom: "bn1"
    top: "relu1"
}

layer_proto {
    name: "conv2"
    type: CONVOLUTION
    bottom: "relu1"
    top: "conv2"
    conv_proto {
        num_output: 32
        kernel_size: 3
    }
}

layer_proto {
    name: "bn2"
    type: BATCH_NORMALIZATION
    bottom: "conv2"
    top: "bn2"
}

layer_proto {
    name: "relu2"
    type: RELU
    bottom: "bn2"
    top: "relu2"
}

layer_proto {
    name: "pool1"
    type: MAX_POOLING
    bottom: "relu2"
    top: "pool1"
    max_pooling_proto {
        win_size: 2
        stride: 2
    }
}

layer_proto {
    name: "conv3"
    type: CONVOLUTION
    bottom: "pool1"
    top: "conv3"
    conv_proto {
        num_output: 64
        kernel_size: 3
    }
}

layer_proto {
    name: "bn3"
    type: BATCH_NORMALIZATION
    bottom: "conv3"
    top: "bn3"
}

layer_proto {
    name: "relu3"
    type: RELU
    bottom: "bn3"
    top: "relu3"
}

layer_proto {
    name: "conv4"
    type: CONVOLUTION
    bottom: "relu3"
    top: "conv4"
    conv_proto {
        num_output: 64
        kernel_size: 3
    }
}

layer_proto {
    name: "bn4"
    type: BATCH_NORMALIZATION
    bottom: "conv4"
    top: "bn4"
}

layer_proto {
    name: "relu4"
    type: RELU
    bottom: "bn4"
    top: "relu4"
}

layer_proto {
    name: "pool2"
    type: MAX_POOLING
    bottom: "relu4"
    top: "pool2"
    max_pooling_proto {
        win_size: 2
        stride: 2
    }
}

layer_proto {
    name: "fc1"
    type: FULL_CONNECTED
    bottom: "pool2"
    top: "fc1"
    fc_proto {
        num_output: 512
    }
}

layer_proto {
    name: "bn5"
    type: BATCH_NORMALIZATION
    bottom: "fc1"
    top: "bn5"
}

layer_proto {
    name: "relu5"
    type: RELU
    bottom: "bn5"
    top: "relu5"
}

layer_proto {
    name: "dropout1"
    type: DROP_OUT
    bottom: "relu5"
    top: "dropout1"
    dropout_proto {
        keep_prob: 0.8
    }
}

layer_proto {
    name: "fc2"
    type: FULL_CONNECTED
    bottom: "dropout1"
    top: "fc2"
    fc_proto {
        num_output: 10
    }
}

layer_proto {
    name: "softmax_log_loss"
    type: SOFTMAX_WITH_LOG_LOSS
    bottom: "fc2"
    bottom: "label"
    top: "loss"
}
